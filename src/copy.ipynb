{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.tensorflow.org/beta/tutorials/quickstart/beginner\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dim(array, t_size):\n",
    "    expanded_array = np.zeros((len(array), t_size[0], t_size[1], 1))\n",
    "    for i, img in enumerate(array):\n",
    "        img_temp = cv2.resize(img, dsize=t_size, interpolation=cv2.INTER_AREA)\n",
    "        img_temp = np.expand_dims(img_temp, axis=-1)\n",
    "        \n",
    "        ## Nomalization\n",
    "        expanded_array[i] = img_temp - .5\n",
    "    \n",
    "    return expanded_array\n",
    "\n",
    "class DataFeeder():\n",
    "    \n",
    "    def __init__(self, dataset, label, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "        \n",
    "        assert dataset.shape[0] == label.shape[0], ...\n",
    "        \"Dataset legnth: {}, Label length: {}\".format(dataset.shape[0], label.shape[0])\n",
    "        \n",
    "        self.dataset_domain = dataset + tf.random.normal(dataset.shape, mean=0.0, stddev=.1)\n",
    "        self.label_domain = label\n",
    "        \n",
    "        self.dataset_size = self.dataset.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batch = int(np.floor(self.dataset_size / self.batch_size))\n",
    "        self.idx_batch = 0\n",
    "        \n",
    "    def feed(self):\n",
    "        \n",
    "        first_idx = self.idx_batch*self.batch_size\n",
    "        if self.idx_batch == self.num_batch:\n",
    "            last_idx = self.dataset_size\n",
    "        else:\n",
    "            last_idx = first_idx + self.batch_size\n",
    "        cur_batch_size = last_idx - first_idx \n",
    "            \n",
    "        data = self.dataset[first_idx:last_idx]\n",
    "        data_expaneded = expand_dim(data, (224, 224))\n",
    "        data_expaneded = tf.cast(data_expaneded, tf.float32)\n",
    "        \n",
    "        domain_data = self.dataset_domain[first_idx:last_idx]\n",
    "        domain_data_expaneded = expand_dim(domain_data, (224, 224))\n",
    "        domain_data_expaneded = tf.cast(domain_data_expaneded, tf.float32)\n",
    "        \n",
    "        class_label = tf.one_hot(self.label[first_idx:last_idx], depth=10)\n",
    "        domain_label = np.concatenate([np.tile(np.asarray([1, 0]), [cur_batch_size, 1]), np.tile(np.asarray([0, 1]), [cur_batch_size, 1])])\n",
    "        \n",
    "        self.idx_batch += 1\n",
    "        if (self.idx_batch >= self.num_batch):\n",
    "            self.shuffle()\n",
    "            self.idx_batch = 0\n",
    "        \n",
    "        outputs = [data_expaneded, domain_data_expaneded, class_label, domain_label]\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def shuffle(self):\n",
    "        labeled_data = list(zip(self.dataset, self.label))\n",
    "        np.random.shuffle(labeled_data)\n",
    "        (self.dataset, self.label) = zip(*labeled_data)\n",
    "        \n",
    "        labeled_domain_data = list(zip(self.dataset_domain, self.label_domain))\n",
    "        np.random.shuffle(labeled_domain_data)\n",
    "        (self.dataset_domain, self.label_domain) = zip(*labeled_data)\n",
    "        \n",
    "        self.idx_batch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntryFlowConvLayer(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(EntryFlowConvLayer, self).__init__()\n",
    "        self.model = tf.keras.Sequential([\n",
    "                        tf.keras.layers.Conv2D(filters=32, \n",
    "                            kernel_size=(3, 3), strides=(2, 2), use_bias=False,\n",
    "                            padding='same', input_shape=input_shape),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu'),\n",
    "                        tf.keras.layers.Conv2D(filters=64, \n",
    "                            kernel_size=(3, 3), strides=(2, 2), use_bias=False,\n",
    "                            padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu')])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "\n",
    "class EntryFlowResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, f_num):\n",
    "        super(EntryFlowResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.SeparableConv2D(filters=f_num, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu'),        \n",
    "                        tf.keras.layers.SeparableConv2D(filters=f_num, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.MaxPooling2D(pool_size=3, \n",
    "                            strides=2, padding='same')])\n",
    "        \n",
    "        self.conv_layer = tf.keras.models.Sequential([\n",
    "                            tf.keras.layers.Conv2D(filters=f_num, \n",
    "                                kernel_size=1,strides=2,padding='same'),\n",
    "                            tf.keras.layers.BatchNormalization()])\n",
    "                           \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs1 = self.conv_layer(inputs)\n",
    "        outputs2 = self.block(inputs)\n",
    "        outputs = tf.add(outputs1, outputs2)\n",
    "        return outputs\n",
    "    \n",
    "class MiddleFlowConvBlock(tf.keras.Model):\n",
    "    def __init__(self, f_num):\n",
    "        super(MiddleFlowConvBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.Activation('relu'), \n",
    "                        tf.keras.layers.SeparableConv2D(filters=f_num, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization()])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.block(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class MiddleFlowResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, f_num):\n",
    "        super(MiddleFlowResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([MiddleFlowConvBlock(f_num) \n",
    "                                                  for i in range(3)])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        results = self.block(inputs)\n",
    "        outputs = tf.add(results, inputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class ExitFlowResidualBlock(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ExitFlowResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.Activation('relu'), \n",
    "                        tf.keras.layers.SeparableConv2D(filters=728, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu'), \n",
    "                        tf.keras.layers.SeparableConv2D(filters=1024, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.MaxPooling2D(pool_size=3, \n",
    "                            strides=2, padding='same')])\n",
    "        \n",
    "        self.conv_layer = tf.keras.models.Sequential([\n",
    "                            tf.keras.layers.Conv2D(filters=1024, \n",
    "                                kernel_size=1,strides=2,padding='same'),\n",
    "                            tf.keras.layers.BatchNormalization()])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs1 = self.block(inputs)\n",
    "        outputs2 = self.conv_layer(inputs)\n",
    "        outputs = tf.add(outputs1, outputs2)\n",
    "        return outputs\n",
    "    \n",
    "class ExitFlowConvLayer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ExitFlowConvLayer, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.SeparableConv2D(filters=1536, \n",
    "                        kernel_size=3, padding='same'),\n",
    "                    tf.keras.layers.BatchNormalization(),\n",
    "                    tf.keras.layers.Activation('relu'), \n",
    "                    tf.keras.layers.SeparableConv2D(filters=2048, \n",
    "                        kernel_size=3, padding='same'),\n",
    "                    tf.keras.layers.BatchNormalization(),\n",
    "                    tf.keras.layers.Activation('relu'), \n",
    "                    tf.keras.layers.GlobalAveragePooling2D()])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class FullyConnectedLayer(tf.keras.Model):\n",
    "    def __init__(self, num_label, k_size):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.Dense(k_size, activation='relu'),\n",
    "                        tf.keras.layers.Dense(k_size, activation='relu'),\n",
    "                        tf.keras.layers.Dense(num_label, activation='softmax')])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "@tf.custom_gradient\n",
    "def gradient_reversal_operator(x):\n",
    "    def grad(dy):\n",
    "        return -1 * dy\n",
    "    return x, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntryFlow(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(EntryFlow, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([EntryFlowConvLayer(input_shape=input_shape),\n",
    "                                               EntryFlowResidualBlock(128),\n",
    "                                               EntryFlowResidualBlock(256),\n",
    "                                               EntryFlowResidualBlock(728)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class MiddleFlow(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MiddleFlow, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([MiddleFlowResidualBlock(728)\n",
    "                                                for i in range(8)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs  \n",
    "    \n",
    "class ExitFlow(tf.keras.Model):\n",
    "    def __init__(self, num_label, k_size):\n",
    "        super(ExitFlow, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([ExitFlowConvLayer(), \n",
    "                                                FullyConnectedLayer(num_label, k_size)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "# class Xception(tf.keras.Model):\n",
    "#     def __init__(self, input_shape):\n",
    "#         super(Xception, self).__init__()\n",
    "# #         self.L1 = tf.keras.losses.CategoricalCrossentropy()\n",
    "# #         self.optimizer = tf.keras.optimizers.Adam()\n",
    "#         self.model = tf.keras.models.Sequential([EntryFlow(input_shape),\n",
    "#                                            MiddleFlow(),\n",
    "#                                            ExitFlow(10)])\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         outputs = self.model(inputs)\n",
    "#         return outputs\n",
    "\n",
    "class Xception():\n",
    "    def __init__(self, input_shape):\n",
    "#         self.L1 = tf.keras.losses.CategoricalCrossentropy()\n",
    "#         self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.feature_extractor = tf.keras.models.Sequential([EntryFlow(input_shape), MiddleFlow(), ExitFlowConvLayer()])\n",
    "        self.label_predictor = FullyConnectedLayer(10, 1024)\n",
    "        self.domain_classifier = FullyConnectedLayer(2, 128)\n",
    "        \n",
    "        self.path1 = tf.keras.models.Sequential([self.feature_extractor, self.label_predictor])\n",
    "        self.path2 = tf.keras.models.Sequential([self.feature_extractor, self.domain_classifier])\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adagrad()\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.class_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.domain_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.train_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    def fit(self, class_inputs, domain_inputs, true):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            prediction = self.predict(class_inputs, domain_inputs)\n",
    "            loss = self.calculate_loss(true, prediction)\n",
    "        self.update_gradient(tape, loss)\n",
    "        del tape\n",
    "        return self.calculate_log(loss, true[0], true[1], prediction[0], prediction[1])\n",
    "    \n",
    "    @tf.function\n",
    "    def predict(self, class_inputs, domain_inputs):\n",
    "        predicted_label = self.path1(class_inputs)   \n",
    "        inputs = tf.concat([class_inputs, domain_inputs], axis=0)\n",
    "        predicted_domain = self.path2(inputs)        \n",
    "        prediction = (predicted_label, predicted_domain)\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, true, prediction):\n",
    "        L1 = self.loss_function(true[0], prediction[0])\n",
    "        L2 = self.loss_function(true[1], prediction[1])\n",
    "        loss = (L1, L2)\n",
    "        return loss\n",
    "\n",
    "    def update_gradient(self, tape, loss):\n",
    "        grad_fy = tape.gradient(loss[0], self.feature_extractor.trainable_variables)\n",
    "        grad_y = tape.gradient(loss[0], self.label_predictor.trainable_variables)\n",
    "        grad_dy = tape.gradient(loss[1], self.feature_extractor.trainable_variables)\n",
    "        grad_d = tape.gradient(loss[1], self.domain_classifier.trainable_variables)\n",
    "        \n",
    "#         print(grad_fy)\n",
    "#         print(grad_y)\n",
    "#         print(grad_dy)\n",
    "#         print(grad_d)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(grad_y, self.label_predictor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(grad_d, self.domain_classifier.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(grad_fy, self.feature_extractor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(grad_dy, self.feature_extractor.trainable_variables))\n",
    "\n",
    "        \n",
    "    def calculate_log(self, loss, c_label, d_label, c_pred, d_pred):\n",
    "#         self.train_loss(loss[0] + loss[1])\n",
    "        self.train_loss(loss[0] + loss[1])\n",
    "        self.class_metric(c_label, c_pred)\n",
    "        self.domain_metric(d_label, d_pred)\n",
    "        log = (self.train_loss.result(), self.class_metric.result(), self.domain_metric.result())\n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feeder = DataFeeder(x_train, y_train, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feeder.shuffle()\n",
    "# plt.imshow(data_feeder.dataset[0])\n",
    "# data_feeder.label[0]\n",
    "# a, b, c, d = data_feeder.feed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(input_shape=(224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "[data, domain_data, class_label, domain_label] = data_feeder.feed()\n",
    "(class_pred, domain_pred) = model.predict(data, domain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ba2304f3fd42c5bd8df0421cbf90ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0625 20:18:05.971215 22284 deprecation.py:323] From c:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(data_feeder.num_batch)):        \n",
    "        [data, domain_data, class_label, domain_label] = data_feeder.feed()\n",
    "#         (class_pred, domain_pred) = model.predict(data, domain_data)\n",
    "#         loss = model.calculate_loss((class_label, domain_label), (class_pred, domain_pred))        \n",
    "#         (m_loss, c_acc, d_acc) = model.calculate_log(loss, class_label, domain_label, class_pred, domain_pred)\n",
    "        \n",
    "        (m_loss, c_acc, d_acc) = model.fit(data, domain_data, (class_label, domain_label))\n",
    "\n",
    "    #     for test_images, test_labels in test_ds:\n",
    "    #         test_step(test_images, test_labels)\n",
    "        if i % 50 == 0:\n",
    "            template = 'Loss: {}, Class Accuracy: {}, Domain Accuracy: {}'\n",
    "            print(template.format(m_loss, c_acc*100, d_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
