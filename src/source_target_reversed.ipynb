{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def GradientReversalOperator(x):\n",
    "    def grad(dy):\n",
    "        return -1 * dy\n",
    "    return x, grad\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return GradientReversalOperator(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST():\n",
    "    def __init__(self, input_shape):\n",
    "        super(MNIST, self).__init__()\n",
    "        self.feature_extractor = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=5,\n",
    "                                   strides=1, input_shape=input_shape),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "            tf.keras.layers.Conv2D(filters=48, kernel_size=5, strides=1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "            tf.keras.layers.Flatten()            \n",
    "        ])\n",
    "        \n",
    "        self.label_predictor = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(100),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(100),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        self.domain_predictor = tf.keras.models.Sequential([\n",
    "            GradientReversalLayer(),\n",
    "            tf.keras.layers.Dense(100),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(2),\n",
    "            tf.keras.layers.Activation('sigmoid')          \n",
    "        ])\n",
    "        self.path_1 = tf.keras.models.Sequential([\n",
    "            self.feature_extractor,\n",
    "            self.label_predictor\n",
    "        ])\n",
    "        self.path_2 = tf.keras.models.Sequential([\n",
    "            self.feature_extractor,\n",
    "            self.domain_predictor\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.loss_2 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.loss_3 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.optimizer_2 = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        self.train_loss = tf.keras.metrics.Mean()\n",
    "        self.train_loss_2 = tf.keras.metrics.Mean()\n",
    "        \n",
    "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.train_accuracy_2 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "        \n",
    "        self.test_loss = tf.keras.metrics.Mean()\n",
    "        self.test_loss_2 = tf.keras.metrics.Mean()\n",
    "        self.test_loss_3 = tf.keras.metrics.Mean()\n",
    "        self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.test_accuracy_2 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.test_accuracy_3 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_both(self, x_class, y_class, x_domain):\n",
    "        \n",
    "        domain_labels = np.concatenate([np.zeros(len(x_class)), np.ones(len(x_domain))])\n",
    "        \n",
    "        x_both = tf.concat([x_class, x_domain], axis = 0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_class_pred = self.path_1(x_class)\n",
    "            loss_1 = self.loss(y_class, y_class_pred)   \n",
    "        grad_1 = tape.gradient(loss_1, self.path_1.trainable_variables)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_domain_pred = self.path_2(x_both)\n",
    "            loss_2 = self.loss_2(domain_labels, y_domain_pred) \n",
    "        grad_2 = tape.gradient(loss_2, self.path_2.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(grad_1, self.path_1.trainable_variables))\n",
    "        self.optimizer_2.apply_gradients(zip(grad_2, self.path_2.trainable_variables))\n",
    "        self.train_loss(loss_1)\n",
    "        self.train_accuracy(y_class, y_class_pred)\n",
    "        \n",
    "        self.train_loss_2(loss_2)\n",
    "        self.train_accuracy_2(domain_labels, y_domain_pred)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    @tf.function\n",
    "    def test_both(self, x_class, y_class, x_domain, y_domain):\n",
    "        \n",
    "        domain_labels = np.concatenate([np.zeros(len(x_class)), np.ones(len(x_domain))])\n",
    "        \n",
    "        x_both = tf.concat([x_class, x_domain], axis = 0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_class_pred = self.path_1(x_class)\n",
    "            y_domain_pred = self.path_2(x_both)\n",
    "            y_target_class_pred = self.path_1(x_domain)\n",
    "            \n",
    "            loss_1 = self.loss(y_class, y_class_pred)\n",
    "            loss_2 = self.loss_2(domain_labels, y_domain_pred)\n",
    "            loss_3 = self.loss_3(y_domain, y_target_class_pred)\n",
    "            \n",
    "        self.test_loss(loss_1)\n",
    "        self.test_accuracy(y_class, y_class_pred)\n",
    "        \n",
    "        self.test_loss_2(loss_2)\n",
    "        self.test_accuracy_2(domain_labels, y_domain_pred)\n",
    "        \n",
    "        self.test_loss_3(loss_3)\n",
    "        self.test_accuracy_3(y_domain, y_target_class_pred)\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHN():\n",
    "    def __init__(self, input_shape):\n",
    "        super(SVHN, self).__init__()\n",
    "        self.feature_extractor = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=5,\n",
    "                                   strides=1, input_shape=input_shape),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),\n",
    "            tf.keras.layers.Conv2D(filters=128, kernel_size=5, strides=1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Flatten()            \n",
    "        ])\n",
    "        \n",
    "        self.label_predictor = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(3072),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(2048),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        self.domain_predictor = tf.keras.models.Sequential([\n",
    "            GradientReversalLayer(),\n",
    "            tf.keras.layers.Dense(1024),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(1024),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(2),\n",
    "            tf.keras.layers.Activation('sigmoid')          \n",
    "        ])\n",
    "        \n",
    "        self.path_1 = tf.keras.models.Sequential([\n",
    "            self.feature_extractor,\n",
    "            self.label_predictor\n",
    "        ])\n",
    "        \n",
    "        self.path_2 = tf.keras.models.Sequential([\n",
    "            self.feature_extractor,\n",
    "            self.domain_predictor\n",
    "        ])\n",
    "        \n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.loss_2 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.loss_3 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.optimizer_2 = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        self.train_loss = tf.keras.metrics.Mean()\n",
    "        self.train_loss_2 = tf.keras.metrics.Mean()\n",
    "        \n",
    "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.train_accuracy_2 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "        \n",
    "        self.test_loss = tf.keras.metrics.Mean()\n",
    "        self.test_loss_2 = tf.keras.metrics.Mean()\n",
    "        self.test_loss_3 = tf.keras.metrics.Mean()\n",
    "        self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.test_accuracy_2 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.test_accuracy_3 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_both(self, x_class, y_class, x_domain):\n",
    "        \n",
    "        domain_labels = np.concatenate([np.zeros(len(x_class)), np.ones(len(x_domain))])\n",
    "        \n",
    "        x_both = tf.concat([x_class, x_domain], axis = 0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_class_pred = self.path_1(x_class)\n",
    "            loss_1 = self.loss(y_class, y_class_pred)   \n",
    "        grad_1 = tape.gradient(loss_1, self.path_1.trainable_variables)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_domain_pred = self.path_2(x_both)\n",
    "            loss_2 = self.loss_2(domain_labels, y_domain_pred) \n",
    "        grad_2 = tape.gradient(loss_2, self.path_2.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(grad_1, self.path_1.trainable_variables))\n",
    "        self.optimizer_2.apply_gradients(zip(grad_2, self.path_2.trainable_variables))\n",
    "        self.train_loss(loss_1)\n",
    "        self.train_accuracy(y_class, y_class_pred)\n",
    "        \n",
    "        self.train_loss_2(loss_2)\n",
    "        self.train_accuracy_2(domain_labels, y_domain_pred)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    @tf.function\n",
    "    def test_both(self, x_class, y_class, x_domain, y_domain):\n",
    "        \n",
    "        domain_labels = np.concatenate([np.zeros(len(x_class)), np.ones(len(x_domain))])\n",
    "        \n",
    "        x_both = tf.concat([x_class, x_domain], axis = 0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_class_pred = self.path_1(x_class)\n",
    "            y_domain_pred = self.path_2(x_both)\n",
    "            y_target_class_pred = self.path_1(x_domain)\n",
    "            \n",
    "            loss_1 = self.loss(y_class, y_class_pred)\n",
    "            loss_2 = self.loss_2(domain_labels, y_domain_pred)\n",
    "            loss_3 = self.loss_3(y_domain, y_target_class_pred)\n",
    "            \n",
    "        self.test_loss(loss_1)\n",
    "        self.test_accuracy(y_class, y_class_pred)\n",
    "        \n",
    "        self.test_loss_2(loss_2)\n",
    "        self.test_accuracy_2(domain_labels, y_domain_pred)\n",
    "        \n",
    "        self.test_loss_3(loss_3)\n",
    "        self.test_accuracy_3(y_domain, y_target_class_pred)\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mnist = np.load('../data/mnist/x_train.npy')\n",
    "y_train_mnist = np.load('../data/mnist/y_train.npy')\n",
    "\n",
    "x_test_mnist = np.load('../data/mnist/x_test.npy')\n",
    "y_test_mnist = np.load('../data/mnist/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_svhn = np.load('../data/svhn/x_train.npy')\n",
    "y_train_svhn = np.load('../data/svhn/y_train.npy')\n",
    "\n",
    "x_test_svhn = np.load('../data/svhn/x_test.npy')\n",
    "y_test_svhn = np.load('../data/svhn/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mnist, x_test_mnist = x_train_mnist / 255.0, x_test_mnist / 255.0\n",
    "x_train_svhn, x_test_svhn = x_train_svhn / 255.0, x_test_svhn / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mnist = tf.cast(x_train_mnist, tf.float32)\n",
    "x_test_mnist = tf.cast(x_test_mnist, tf.float32)\n",
    "x_train_svhn = tf.cast(x_train_svhn, tf.float32)\n",
    "x_test_svhn = tf.cast(x_test_svhn, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image(x, y):\n",
    "    \n",
    "    paddings = tf.constant([[2, 2,], [2, 2]])\n",
    "    \n",
    "    new_x = tf.pad(x, paddings, \"CONSTANT\")\n",
    "    \n",
    "    return (new_x, y)\n",
    "\n",
    "def duplicate_channel(x, y):\n",
    "\n",
    "    new_x = tf.stack([x, x, x], axis = -1)\n",
    "    \n",
    "    return (new_x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_dataset = tf.data.Dataset.from_tensor_slices((x_train_mnist, y_train_mnist))\n",
    "mnist_train_dataset = mnist_train_dataset.map(pad_image)\n",
    "mnist_train_dataset = mnist_train_dataset.map(duplicate_channel)\n",
    "target_train_dataset = mnist_train_dataset.shuffle(len(y_train_mnist))\n",
    "\n",
    "svhn_train_dataset = tf.data.Dataset.from_tensor_slices((x_train_svhn, y_train_svhn))\n",
    "source_train_dataset = svhn_train_dataset.shuffle(len(y_train_svhn))\n",
    "\n",
    "source_train_dataset = source_train_dataset.batch(2000, drop_remainder=True)\n",
    "source_train_dataset = source_train_dataset.prefetch(50)\n",
    "\n",
    "target_train_dataset = target_train_dataset.batch(2000, drop_remainder=True)\n",
    "target_train_dataset = target_train_dataset.prefetch(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST(input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0627 18:02:12.008243 21964 deprecation.py:323] From c:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "L1: 2.2375, Acc1: 18.72, L1 Test: 2.2102, Acc1 Test: 19.45\n",
      "L2: 0.6681, Acc2: 65.06, L2 Test: 0.6302, Acc2 Test: 67.12\n",
      "L3 Test: 2.3877, Acc3 Test: 12.26\n",
      "\n",
      "Epoch: 2\n",
      "L1: 2.1624, Acc1: 22.43, L1 Test: 2.0289, Acc1 Test: 31.21\n",
      "L2: 0.5962, Acc2: 71.33, L2 Test: 0.4993, Acc2 Test: 78.63\n",
      "L3 Test: 2.3771, Acc3 Test: 18.23\n",
      "\n",
      "Epoch: 3\n",
      "L1: 1.9542, Acc1: 32.01, L1 Test: 1.7546, Acc1 Test: 42.22\n",
      "L2: 0.4778, Acc2: 79.50, L2 Test: 0.3797, Acc2 Test: 85.15\n",
      "L3 Test: 2.7375, Acc3 Test: 22.68\n",
      "\n",
      "Epoch: 4\n",
      "L1: 1.7208, Acc1: 41.30, L1 Test: 1.5332, Acc1 Test: 50.45\n",
      "L2: 0.3842, Acc2: 84.31, L2 Test: 0.3077, Acc2 Test: 88.54\n",
      "L3 Test: 3.1489, Acc3 Test: 26.85\n",
      "\n",
      "Epoch: 5\n",
      "L1: 1.5345, Acc1: 48.49, L1 Test: 1.3685, Acc1 Test: 56.33\n",
      "L2: 0.3217, Acc2: 87.26, L2 Test: 0.2588, Acc2 Test: 90.67\n",
      "L3 Test: 3.4101, Acc3 Test: 30.64\n",
      "\n",
      "Epoch: 6\n",
      "L1: 1.3904, Acc1: 53.91, L1 Test: 1.2443, Acc1 Test: 60.72\n",
      "L2: 0.2768, Acc2: 89.27, L2 Test: 0.2239, Acc2 Test: 92.12\n",
      "L3 Test: 3.5665, Acc3 Test: 33.66\n",
      "\n",
      "Epoch: 7\n",
      "L1: 1.2774, Acc1: 58.09, L1 Test: 1.1473, Acc1 Test: 64.09\n",
      "L2: 0.2431, Acc2: 90.73, L2 Test: 0.1974, Acc2 Test: 93.17\n",
      "L3 Test: 3.6458, Acc3 Test: 36.14\n",
      "\n",
      "Epoch: 8\n",
      "L1: 1.1866, Acc1: 61.42, L1 Test: 1.0695, Acc1 Test: 66.76\n",
      "L2: 0.2170, Acc2: 91.83, L2 Test: 0.1770, Acc2 Test: 93.97\n",
      "L3 Test: 3.6784, Acc3 Test: 38.22\n",
      "\n",
      "Epoch: 9\n",
      "L1: 1.1121, Acc1: 64.12, L1 Test: 1.0057, Acc1 Test: 68.93\n",
      "L2: 0.1963, Acc2: 92.69, L2 Test: 0.1609, Acc2 Test: 94.59\n",
      "L3 Test: 3.7000, Acc3 Test: 39.85\n",
      "\n",
      "Epoch: 10\n",
      "L1: 1.0498, Acc1: 66.35, L1 Test: 0.9523, Acc1 Test: 70.72\n",
      "L2: 0.1796, Acc2: 93.38, L2 Test: 0.1481, Acc2 Test: 95.08\n",
      "L3 Test: 3.6812, Acc3 Test: 41.35\n",
      "\n",
      "Epoch: 11\n",
      "L1: 0.9970, Acc1: 68.22, L1 Test: 0.9069, Acc1 Test: 72.25\n",
      "L2: 0.1663, Acc2: 93.93, L2 Test: 0.1381, Acc2 Test: 95.46\n",
      "L3 Test: 3.6546, Acc3 Test: 42.59\n",
      "\n",
      "Epoch: 12\n",
      "L1: 0.9514, Acc1: 69.83, L1 Test: 0.8674, Acc1 Test: 73.55\n",
      "L2: 0.1555, Acc2: 94.37, L2 Test: 0.1296, Acc2 Test: 95.77\n",
      "L3 Test: 3.6126, Acc3 Test: 43.70\n",
      "\n",
      "Epoch: 13\n",
      "L1: 0.9110, Acc1: 71.24, L1 Test: 0.8325, Acc1 Test: 74.69\n",
      "L2: 0.1466, Acc2: 94.74, L2 Test: 0.1227, Acc2 Test: 96.02\n",
      "L3 Test: 3.5638, Acc3 Test: 44.64\n",
      "\n",
      "Epoch: 14\n",
      "L1: 0.8753, Acc1: 72.47, L1 Test: 0.8017, Acc1 Test: 75.69\n",
      "L2: 0.1395, Acc2: 95.03, L2 Test: 0.1182, Acc2 Test: 96.19\n",
      "L3 Test: 3.5075, Acc3 Test: 45.46\n",
      "\n",
      "Epoch: 15\n",
      "L1: 0.8435, Acc1: 73.57, L1 Test: 0.7740, Acc1 Test: 76.58\n",
      "L2: 0.1353, Acc2: 95.22, L2 Test: 0.1173, Acc2 Test: 96.21\n",
      "L3 Test: 3.4599, Acc3 Test: 46.05\n",
      "\n",
      "Epoch: 16\n",
      "L1: 0.8151, Acc1: 74.53, L1 Test: 0.7494, Acc1 Test: 77.36\n",
      "L2: 0.1345, Acc2: 95.25, L2 Test: 0.1195, Acc2 Test: 96.10\n",
      "L3 Test: 3.4262, Acc3 Test: 46.47\n",
      "\n",
      "Epoch: 17\n",
      "L1: 0.7894, Acc1: 75.40, L1 Test: 0.7267, Acc1 Test: 78.08\n",
      "L2: 0.1350, Acc2: 95.23, L2 Test: 0.1199, Acc2 Test: 96.09\n",
      "L3 Test: 3.3917, Acc3 Test: 46.81\n",
      "\n",
      "Epoch: 18\n",
      "L1: 0.7660, Acc1: 76.18, L1 Test: 0.7058, Acc1 Test: 78.74\n",
      "L2: 0.1331, Acc2: 95.32, L2 Test: 0.1180, Acc2 Test: 96.17\n",
      "L3 Test: 3.3614, Acc3 Test: 47.13\n",
      "\n",
      "Epoch: 19\n",
      "L1: 0.7444, Acc1: 76.89, L1 Test: 0.6869, Acc1 Test: 79.34\n",
      "L2: 0.1304, Acc2: 95.45, L2 Test: 0.1160, Acc2 Test: 96.25\n",
      "L3 Test: 3.3347, Acc3 Test: 47.47\n",
      "\n",
      "Epoch: 20\n",
      "L1: 0.7243, Acc1: 77.56, L1 Test: 0.6696, Acc1 Test: 79.87\n",
      "L2: 0.1279, Acc2: 95.56, L2 Test: 0.1141, Acc2 Test: 96.34\n",
      "L3 Test: 3.3061, Acc3 Test: 47.80\n",
      "\n",
      "Epoch: 21\n",
      "L1: 0.7056, Acc1: 78.17, L1 Test: 0.6533, Acc1 Test: 80.37\n",
      "L2: 0.1251, Acc2: 95.68, L2 Test: 0.1116, Acc2 Test: 96.43\n",
      "L3 Test: 3.2796, Acc3 Test: 48.10\n",
      "\n",
      "Epoch: 22\n",
      "L1: 0.6882, Acc1: 78.74, L1 Test: 0.6382, Acc1 Test: 80.85\n",
      "L2: 0.1220, Acc2: 95.81, L2 Test: 0.1092, Acc2 Test: 96.52\n",
      "L3 Test: 3.2587, Acc3 Test: 48.35\n",
      "\n",
      "Epoch: 23\n",
      "L1: 0.6720, Acc1: 79.27, L1 Test: 0.6239, Acc1 Test: 81.29\n",
      "L2: 0.1190, Acc2: 95.93, L2 Test: 0.1070, Acc2 Test: 96.61\n",
      "L3 Test: 3.2445, Acc3 Test: 48.56\n",
      "\n",
      "Epoch: 24\n",
      "L1: 0.6570, Acc1: 79.75, L1 Test: 0.6104, Acc1 Test: 81.71\n",
      "L2: 0.1162, Acc2: 96.04, L2 Test: 0.1048, Acc2 Test: 96.68\n",
      "L3 Test: 3.2378, Acc3 Test: 48.75\n",
      "\n",
      "Epoch: 25\n",
      "L1: 0.6429, Acc1: 80.21, L1 Test: 0.5981, Acc1 Test: 82.08\n",
      "L2: 0.1136, Acc2: 96.14, L2 Test: 0.1027, Acc2 Test: 96.76\n",
      "L3 Test: 3.2312, Acc3 Test: 48.92\n",
      "\n",
      "Epoch: 26\n",
      "L1: 0.6296, Acc1: 80.64, L1 Test: 0.5861, Acc1 Test: 82.46\n",
      "L2: 0.1112, Acc2: 96.24, L2 Test: 0.1006, Acc2 Test: 96.83\n",
      "L3 Test: 3.2293, Acc3 Test: 49.06\n",
      "\n",
      "Epoch: 27\n",
      "L1: 0.6170, Acc1: 81.05, L1 Test: 0.5748, Acc1 Test: 82.81\n",
      "L2: 0.1088, Acc2: 96.34, L2 Test: 0.0984, Acc2 Test: 96.91\n",
      "L3 Test: 3.2300, Acc3 Test: 49.16\n",
      "\n",
      "Epoch: 28\n",
      "L1: 0.6049, Acc1: 81.44, L1 Test: 0.5639, Acc1 Test: 83.14\n",
      "L2: 0.1064, Acc2: 96.43, L2 Test: 0.0962, Acc2 Test: 96.98\n",
      "L3 Test: 3.2285, Acc3 Test: 49.26\n",
      "\n",
      "Epoch: 29\n",
      "L1: 0.5935, Acc1: 81.81, L1 Test: 0.5537, Acc1 Test: 83.45\n",
      "L2: 0.1039, Acc2: 96.52, L2 Test: 0.0940, Acc2 Test: 97.06\n",
      "L3 Test: 3.2280, Acc3 Test: 49.36\n",
      "\n",
      "Epoch: 30\n",
      "L1: 0.5827, Acc1: 82.16, L1 Test: 0.5442, Acc1 Test: 83.74\n",
      "L2: 0.1015, Acc2: 96.61, L2 Test: 0.0918, Acc2 Test: 97.13\n",
      "L3 Test: 3.2307, Acc3 Test: 49.43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    i = 0\n",
    "    for (source_images, class_labels), (target_images, _) in zip(source_train_dataset, target_train_dataset):\n",
    "        model.train_both(source_images, class_labels, target_images)\n",
    "        print(i)\n",
    "        i =\n",
    "\n",
    "    for (test_images, test_labels), (target_images, target_labels) in zip(source_train_dataset, target_train_dataset):\n",
    "        model.test_both(test_images, test_labels, target_images, target_labels)\n",
    "\n",
    "    template = 'Epoch: {}\\n' + \\\n",
    "    'L1: {:.4f}, Acc1: {:.2f}, L1 Test: {:.4f}, Acc1 Test: {:.2f}\\n'+ \\\n",
    "    'L2: {:.4f}, Acc2: {:.2f}, L2 Test: {:.4f}, Acc2 Test: {:.2f}\\n'+ \\\n",
    "    'L3 Test: {:.4f}, Acc3 Test: {:.2f}\\n'\n",
    "    \n",
    "    \n",
    "    print(template.format(epoch+1,\n",
    "                         model.train_loss.result(),\n",
    "                         model.train_accuracy.result()*100,\n",
    "                         model.test_loss.result(),\n",
    "                         model.test_accuracy.result()*100,\n",
    "                         model.train_loss_2.result(),\n",
    "                         model.train_accuracy_2.result()*100,\n",
    "                         model.test_loss_2.result(),\n",
    "                         model.test_accuracy_2.result()*100,\n",
    "                         model.test_loss_3.result(),\n",
    "                         model.test_accuracy_3.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "L1: 0.5726, Acc1: 82.48, L1 Test: 0.5353, Acc1 Test: 84.01\n",
      "L2: 0.0991, Acc2: 96.70, L2 Test: 0.0896, Acc2 Test: 97.21\n",
      "L3 Test: 3.2361, Acc3 Test: 49.47\n",
      "\n",
      "Epoch: 2\n",
      "L1: 0.5629, Acc1: 82.79, L1 Test: 0.5272, Acc1 Test: 84.25\n",
      "L2: 0.0968, Acc2: 96.78, L2 Test: 0.0874, Acc2 Test: 97.28\n",
      "L3 Test: 3.2425, Acc3 Test: 49.49\n",
      "\n",
      "Epoch: 3\n",
      "L1: 0.5537, Acc1: 83.08, L1 Test: 0.5192, Acc1 Test: 84.49\n",
      "L2: 0.0945, Acc2: 96.87, L2 Test: 0.0853, Acc2 Test: 97.35\n",
      "L3 Test: 3.2532, Acc3 Test: 49.52\n",
      "\n",
      "Epoch: 4\n",
      "L1: 0.5449, Acc1: 83.36, L1 Test: 0.5113, Acc1 Test: 84.73\n",
      "L2: 0.0922, Acc2: 96.95, L2 Test: 0.0834, Acc2 Test: 97.41\n",
      "L3 Test: 3.2750, Acc3 Test: 49.51\n",
      "\n",
      "Epoch: 5\n",
      "L1: 0.5366, Acc1: 83.62, L1 Test: 0.5036, Acc1 Test: 84.97\n",
      "L2: 0.0901, Acc2: 97.02, L2 Test: 0.0816, Acc2 Test: 97.47\n",
      "L3 Test: 3.2976, Acc3 Test: 49.52\n",
      "\n",
      "Epoch: 6\n",
      "L1: 0.5286, Acc1: 83.87, L1 Test: 0.4961, Acc1 Test: 85.20\n",
      "L2: 0.0880, Acc2: 97.09, L2 Test: 0.0798, Acc2 Test: 97.53\n",
      "L3 Test: 3.3233, Acc3 Test: 49.49\n",
      "\n",
      "Epoch: 7\n",
      "L1: 0.5207, Acc1: 84.12, L1 Test: 0.4891, Acc1 Test: 85.41\n",
      "L2: 0.0860, Acc2: 97.16, L2 Test: 0.0780, Acc2 Test: 97.59\n",
      "L3 Test: 3.3535, Acc3 Test: 49.45\n",
      "\n",
      "Epoch: 8\n",
      "L1: 0.5132, Acc1: 84.36, L1 Test: 0.4823, Acc1 Test: 85.62\n",
      "L2: 0.0841, Acc2: 97.23, L2 Test: 0.0763, Acc2 Test: 97.65\n",
      "L3 Test: 3.3788, Acc3 Test: 49.40\n",
      "\n",
      "Epoch: 9\n",
      "L1: 0.5061, Acc1: 84.58, L1 Test: 0.4762, Acc1 Test: 85.80\n",
      "L2: 0.0822, Acc2: 97.29, L2 Test: 0.0747, Acc2 Test: 97.70\n",
      "L3 Test: 3.4045, Acc3 Test: 49.37\n",
      "\n",
      "Epoch: 10\n",
      "L1: 0.4992, Acc1: 84.80, L1 Test: 0.4698, Acc1 Test: 85.99\n",
      "L2: 0.0805, Acc2: 97.35, L2 Test: 0.0731, Acc2 Test: 97.75\n",
      "L3 Test: 3.4399, Acc3 Test: 49.34\n",
      "\n",
      "Epoch: 11\n",
      "L1: 0.4926, Acc1: 85.01, L1 Test: 0.4636, Acc1 Test: 86.18\n",
      "L2: 0.0788, Acc2: 97.41, L2 Test: 0.0715, Acc2 Test: 97.80\n",
      "L3 Test: 3.4621, Acc3 Test: 49.32\n",
      "\n",
      "Epoch: 12\n",
      "L1: 0.4861, Acc1: 85.21, L1 Test: 0.4579, Acc1 Test: 86.35\n",
      "L2: 0.0771, Acc2: 97.47, L2 Test: 0.0700, Acc2 Test: 97.85\n",
      "L3 Test: 3.4929, Acc3 Test: 49.24\n",
      "\n",
      "Epoch: 13\n",
      "L1: 0.4798, Acc1: 85.40, L1 Test: 0.4522, Acc1 Test: 86.52\n",
      "L2: 0.0755, Acc2: 97.52, L2 Test: 0.0686, Acc2 Test: 97.89\n",
      "L3 Test: 3.5253, Acc3 Test: 49.17\n",
      "\n",
      "Epoch: 14\n",
      "L1: 0.4737, Acc1: 85.60, L1 Test: 0.4467, Acc1 Test: 86.69\n",
      "L2: 0.0739, Acc2: 97.57, L2 Test: 0.0671, Acc2 Test: 97.94\n",
      "L3 Test: 3.5538, Acc3 Test: 49.11\n",
      "\n",
      "Epoch: 15\n",
      "L1: 0.4678, Acc1: 85.78, L1 Test: 0.4413, Acc1 Test: 86.85\n",
      "L2: 0.0724, Acc2: 97.62, L2 Test: 0.0658, Acc2 Test: 97.98\n",
      "L3 Test: 3.5812, Acc3 Test: 49.05\n",
      "\n",
      "Epoch: 16\n",
      "L1: 0.4619, Acc1: 85.96, L1 Test: 0.4361, Acc1 Test: 87.00\n",
      "L2: 0.0710, Acc2: 97.67, L2 Test: 0.0645, Acc2 Test: 98.02\n",
      "L3 Test: 3.6122, Acc3 Test: 48.99\n",
      "\n",
      "Epoch: 17\n",
      "L1: 0.4562, Acc1: 86.14, L1 Test: 0.4310, Acc1 Test: 87.16\n",
      "L2: 0.0696, Acc2: 97.72, L2 Test: 0.0633, Acc2 Test: 98.06\n",
      "L3 Test: 3.6480, Acc3 Test: 48.92\n",
      "\n",
      "Epoch: 18\n",
      "L1: 0.4507, Acc1: 86.31, L1 Test: 0.4260, Acc1 Test: 87.31\n",
      "L2: 0.0683, Acc2: 97.76, L2 Test: 0.0621, Acc2 Test: 98.10\n",
      "L3 Test: 3.6862, Acc3 Test: 48.82\n",
      "\n",
      "Epoch: 19\n",
      "L1: 0.4454, Acc1: 86.48, L1 Test: 0.4212, Acc1 Test: 87.45\n",
      "L2: 0.0670, Acc2: 97.81, L2 Test: 0.0609, Acc2 Test: 98.13\n",
      "L3 Test: 3.7199, Acc3 Test: 48.74\n",
      "\n",
      "Epoch: 20\n",
      "L1: 0.4401, Acc1: 86.65, L1 Test: 0.4167, Acc1 Test: 87.58\n",
      "L2: 0.0657, Acc2: 97.85, L2 Test: 0.0597, Acc2 Test: 98.17\n",
      "L3 Test: 3.7545, Acc3 Test: 48.65\n",
      "\n",
      "Epoch: 21\n",
      "L1: 0.4351, Acc1: 86.80, L1 Test: 0.4122, Acc1 Test: 87.72\n",
      "L2: 0.0645, Acc2: 97.89, L2 Test: 0.0587, Acc2 Test: 98.20\n",
      "L3 Test: 3.7955, Acc3 Test: 48.56\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0650611cc917>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msource_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_train_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_both\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_train_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \"\"\"\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2134\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m         \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2136\u001b[1;33m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2137\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for (source_images, class_labels), (target_images, _) in zip(source_train_dataset, target_train_dataset):\n",
    "        model.train_both(source_images, class_labels, target_images)\n",
    "\n",
    "    for (test_images, test_labels), (target_images, target_labels) in zip(source_train_dataset, target_train_dataset):\n",
    "        model.test_both(test_images, test_labels, target_images, target_labels)\n",
    "\n",
    "    template = 'Epoch: {}\\n' + \\\n",
    "    'L1: {:.4f}, Acc1: {:.2f}, L1 Test: {:.4f}, Acc1 Test: {:.2f}\\n'+ \\\n",
    "    'L2: {:.4f}, Acc2: {:.2f}, L2 Test: {:.4f}, Acc2 Test: {:.2f}\\n'+ \\\n",
    "    'L3 Test: {:.4f}, Acc3 Test: {:.2f}\\n'\n",
    "    \n",
    "    \n",
    "    print(template.format(epoch+1,\n",
    "                         model.train_loss.result(),\n",
    "                         model.train_accuracy.result()*100,\n",
    "                         model.test_loss.result(),\n",
    "                         model.test_accuracy.result()*100,\n",
    "                         model.train_loss_2.result(),\n",
    "                         model.train_accuracy_2.result()*100,\n",
    "                         model.test_loss_2.result(),\n",
    "                         model.test_accuracy_2.result()*100,\n",
    "                         model.test_loss_3.result(),\n",
    "                         model.test_accuracy_3.result()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
