{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.tensorflow.org/beta/tutorials/quickstart/beginner\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dim(array, t_size):\n",
    "    expanded_array = np.zeros((len(array), t_size[0], t_size[1], 1))\n",
    "    for i, img in enumerate(array):\n",
    "        img_temp = cv2.resize(img, dsize=t_size, interpolation=cv2.INTER_AREA)\n",
    "        img_temp = np.expand_dims(img_temp, axis=-1)\n",
    "        \n",
    "        ## Nomalization\n",
    "        expanded_array[i] = img_temp - .5\n",
    "    \n",
    "    return expanded_array\n",
    "\n",
    "class DataFeeder():\n",
    "    \n",
    "    def __init__(self, dataset, label, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "        \n",
    "        assert dataset.shape[0] == label.shape[0], ...\n",
    "        \"Dataset legnth: {}, Label length: {}\".format(dataset.shape[0], label.shape[0])\n",
    "        \n",
    "        self.dataset_domain = dataset + tf.random.normal(dataset.shape, mean=0.0, stddev=.1)\n",
    "        self.label_domain = label\n",
    "        \n",
    "        self.dataset_size = self.dataset.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batch = int(np.floor(self.dataset_size / self.batch_size))\n",
    "        self.idx_batch = 0\n",
    "        \n",
    "    def feed(self):\n",
    "        \n",
    "        first_idx = self.idx_batch*self.batch_size\n",
    "        if self.idx_batch == self.num_batch:\n",
    "            last_idx = self.dataset_size\n",
    "        else:\n",
    "            last_idx = first_idx + self.batch_size\n",
    "        cur_batch_size = last_idx - first_idx \n",
    "            \n",
    "        data = self.dataset[first_idx:last_idx]\n",
    "        data_expaneded = expand_dim(data, (224, 224))\n",
    "        data_expaneded = tf.cast(data_expaneded, tf.float32)\n",
    "        \n",
    "        domain_data = self.dataset_domain[first_idx:last_idx]\n",
    "        domain_data_expaneded = expand_dim(domain_data, (224, 224))\n",
    "        domain_data_expaneded = tf.cast(domain_data_expaneded, tf.float32)\n",
    "        \n",
    "        class_label = tf.one_hot(self.label[first_idx:last_idx], depth=10)\n",
    "        domain_label = np.concatenate([np.tile(np.asarray([1, 0]), [cur_batch_size, 1]), np.tile(np.asarray([0, 1]), [cur_batch_size, 1])])\n",
    "        \n",
    "        self.idx_batch += 1\n",
    "        if (self.idx_batch >= self.num_batch):\n",
    "            self.shuffle()\n",
    "            self.idx_batch = 0\n",
    "        \n",
    "        outputs = [data_expaneded, domain_data_expaneded, class_label, domain_label]\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def shuffle(self):\n",
    "        labeled_data = list(zip(self.dataset, self.label))\n",
    "        np.random.shuffle(labeled_data)\n",
    "        (self.dataset, self.label) = zip(*labeled_data)\n",
    "        \n",
    "        labeled_domain_data = list(zip(self.dataset_domain, self.label_domain))\n",
    "        np.random.shuffle(labeled_domain_data)\n",
    "        (self.dataset_domain, self.label_domain) = zip(*labeled_data)\n",
    "        \n",
    "        self.idx_batch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntryFlowConvLayer(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(EntryFlowConvLayer, self).__init__()\n",
    "        self.model = tf.keras.Sequential([\n",
    "                        tf.keras.layers.Conv2D(filters=32, \n",
    "                            kernel_size=(3, 3), strides=(2, 2), use_bias=False,\n",
    "                            padding='same', input_shape=input_shape),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu'),\n",
    "                        tf.keras.layers.Conv2D(filters=64, \n",
    "                            kernel_size=(3, 3), strides=(2, 2), use_bias=False,\n",
    "                            padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu')])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "\n",
    "class EntryFlowResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, f_num):\n",
    "        super(EntryFlowResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.SeparableConv2D(filters=f_num, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu'),        \n",
    "                        tf.keras.layers.SeparableConv2D(filters=f_num, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.MaxPooling2D(pool_size=3, \n",
    "                            strides=2, padding='same')])\n",
    "        \n",
    "        self.conv_layer = tf.keras.models.Sequential([\n",
    "                            tf.keras.layers.Conv2D(filters=f_num, \n",
    "                                kernel_size=1,strides=2,padding='same'),\n",
    "                            tf.keras.layers.BatchNormalization()])\n",
    "                           \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs1 = self.conv_layer(inputs)\n",
    "        outputs2 = self.block(inputs)\n",
    "        outputs = tf.add(outputs1, outputs2)\n",
    "        return outputs\n",
    "    \n",
    "class MiddleFlowConvBlock(tf.keras.Model):\n",
    "    def __init__(self, f_num):\n",
    "        super(MiddleFlowConvBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.Activation('relu'), \n",
    "                        tf.keras.layers.SeparableConv2D(filters=f_num, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization()])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.block(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class MiddleFlowResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, f_num):\n",
    "        super(MiddleFlowResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([MiddleFlowConvBlock(f_num) \n",
    "                                                  for i in range(3)])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        results = self.block(inputs)\n",
    "        outputs = tf.add(results, inputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class ExitFlowResidualBlock(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ExitFlowResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.Activation('relu'), \n",
    "                        tf.keras.layers.SeparableConv2D(filters=728, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Activation('relu'), \n",
    "                        tf.keras.layers.SeparableConv2D(filters=1024, \n",
    "                            kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.MaxPooling2D(pool_size=3, \n",
    "                            strides=2, padding='same')])\n",
    "        \n",
    "        self.conv_layer = tf.keras.models.Sequential([\n",
    "                            tf.keras.layers.Conv2D(filters=1024, \n",
    "                                kernel_size=1,strides=2,padding='same'),\n",
    "                            tf.keras.layers.BatchNormalization()])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs1 = self.block(inputs)\n",
    "        outputs2 = self.conv_layer(inputs)\n",
    "        outputs = tf.add(outputs1, outputs2)\n",
    "        return outputs\n",
    "    \n",
    "class ExitFlowConvLayer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ExitFlowConvLayer, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.SeparableConv2D(filters=1536, \n",
    "                        kernel_size=3, padding='same'),\n",
    "                    tf.keras.layers.BatchNormalization(),\n",
    "                    tf.keras.layers.Activation('relu'), \n",
    "                    tf.keras.layers.SeparableConv2D(filters=2048, \n",
    "                        kernel_size=3, padding='same'),\n",
    "                    tf.keras.layers.BatchNormalization(),\n",
    "                    tf.keras.layers.Activation('relu'), \n",
    "                    tf.keras.layers.GlobalAveragePooling2D()])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class FullyConnectedLayer(tf.keras.Model):\n",
    "    def __init__(self, num_label, k_size):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                        tf.keras.layers.Dense(k_size, activation='relu'),\n",
    "                        tf.keras.layers.Dense(k_size, activation='relu'),\n",
    "                        tf.keras.layers.Dense(num_label, activation='softmax')])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "@tf.custom_gradient\n",
    "def gradient_reversal_op(x):\n",
    "    def grad(dy):\n",
    "        return -1 * dy\n",
    "    return x, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntryFlow(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(EntryFlow, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([EntryFlowConvLayer(input_shape=input_shape),\n",
    "                                               EntryFlowResidualBlock(128),\n",
    "                                               EntryFlowResidualBlock(256),\n",
    "                                               EntryFlowResidualBlock(728)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class MiddleFlow(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MiddleFlow, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([MiddleFlowResidualBlock(728)\n",
    "                                                for i in range(8)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs  \n",
    "    \n",
    "class ExitFlow(tf.keras.Model):\n",
    "    def __init__(self, num_label, k_size):\n",
    "        super(ExitFlow, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([ExitFlowConvLayer(), \n",
    "                                                FullyConnectedLayer(num_label, k_size)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "# class Xception(tf.keras.Model):\n",
    "#     def __init__(self, input_shape):\n",
    "#         super(Xception, self).__init__()\n",
    "# #         self.L1 = tf.keras.losses.CategoricalCrossentropy()\n",
    "# #         self.optimizer = tf.keras.optimizers.Adam()\n",
    "#         self.model = tf.keras.models.Sequential([EntryFlow(input_shape),\n",
    "#                                            MiddleFlow(),\n",
    "#                                            ExitFlow(10)])\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         outputs = self.model(inputs)\n",
    "#         return outputs\n",
    "\n",
    "class Xception():\n",
    "    def __init__(self, input_shape):\n",
    "#         self.L1 = tf.keras.losses.CategoricalCrossentropy()\n",
    "#         self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.feature_extractor = tf.keras.models.Sequential([EntryFlow(input_shape),\n",
    "                                           MiddleFlow()])\n",
    "        self.label_predictor = ExitFlow(10, 1024)\n",
    "        self.domain_classifier = ExitFlow(2, 128)\n",
    "        self.optimizer = tf.keras.optimizers.Adagrad()\n",
    "        self.loss_function = loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.class_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.domain_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.train_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    @tf.function\n",
    "    def fit(self, class_inputs, domain_inputs, true):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            prediction = self.predict(class_inputs, domain_inputs)\n",
    "            loss = self.calculate_loss(true, prediction)\n",
    "        self.update_gradient(tape, loss)\n",
    "        del tape\n",
    "        return self.calculate_log(loss, true[0], true[1], prediction[0], prediction[1])\n",
    "    \n",
    "    @tf.function\n",
    "    def predict(self, class_inputs, domain_inputs):\n",
    "        class_features = self.feature_extractor(class_inputs)\n",
    "        predicted_label = self.label_predictor(class_features)\n",
    "        domain_features = self.feature_extractor(domain_inputs)\n",
    "        features = tf.concat([class_features, domain_features], axis=0)\n",
    "        predicted_domain = self.domain_classifier(features)        \n",
    "        prediction = (predicted_label, predicted_domain)\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, true, prediction):\n",
    "        L1 = self.loss_function(true[0], prediction[0])\n",
    "        L2 = self.loss_function(true[1], prediction[1])\n",
    "        loss = (L1, L2)\n",
    "        return loss\n",
    "    \n",
    "    def update_gradient(self, tape, loss):\n",
    "        print(loss[0])\n",
    "        print(loss[1])\n",
    "        grad_fy = tape.gradient(loss[0], self.feature_extractor.trainable_variables)\n",
    "        print(len(grad_fy))\n",
    "        grad_y = tape.gradient(loss[0], self.label_predictor.trainable_variables)\n",
    "        print(len(grad_y))\n",
    "        grad_dy = tape.gradient(loss[1], self.feature_extractor.trainable_variables)\n",
    "        print(len(grad_dy))\n",
    "        grad_d = tape.gradient(loss[1], self.domain_classifier.trainable_variables)\n",
    "        print(len(grad_d))\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(grad_y, self.label_predictor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(grad_d, self.domain_classifier.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(grad_fy, self.feature_extractor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(grad_dy, self.feature_extractor.trainable_variables))\n",
    "\n",
    "        \n",
    "    def calculate_log(self, loss, c_label, d_label, c_pred, d_pred):\n",
    "        self.train_loss(loss[0] + loss[1])\n",
    "        self.class_metric(c_label, c_pred)\n",
    "        self.domain_metric(d_label, d_pred)\n",
    "        log = (self.train_loss.result(), self.class_metric.result(), self.domain_metric.result())\n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feeder = DataFeeder(x_train, y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feeder.shuffle()\n",
    "# plt.imshow(data_feeder.dataset[0])\n",
    "# data_feeder.label[0]\n",
    "# a, b, c, d = data_feeder.feed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(input_shape=(224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992d93c3d881465ba2b8fc84e999b563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"categorical_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"categorical_crossentropy_1/weighted_loss/value:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0625 19:06:15.640768 29856 deprecation.py:323] From c:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "16\n",
      "168\n",
      "16\n",
      "Tensor(\"categorical_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"categorical_crossentropy_1/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "168\n",
      "16\n",
      "168\n",
      "16\n",
      "Loss: 2.995732069015503, Class Accuracy: 8.0, Domain Accuracy: 50.0\n",
      "Loss: 2.9957265853881836, Class Accuracy: 10.0, Domain Accuracy: 50.0\n",
      "Loss: 2.9957361221313477, Class Accuracy: 9.333333015441895, Domain Accuracy: 50.0\n",
      "Loss: 2.995732069015503, Class Accuracy: 10.5, Domain Accuracy: 50.0\n",
      "Loss: 2.9957334995269775, Class Accuracy: 10.0, Domain Accuracy: 50.0\n",
      "Loss: 2.9957220554351807, Class Accuracy: 10.666666984558105, Domain Accuracy: 50.0\n",
      "Loss: 2.9957356452941895, Class Accuracy: 10.571428298950195, Domain Accuracy: 50.0\n",
      "Loss: 2.9957265853881836, Class Accuracy: 10.75, Domain Accuracy: 50.0\n",
      "Loss: 2.9957313537597656, Class Accuracy: 10.44444465637207, Domain Accuracy: 50.0\n",
      "Loss: 2.995723009109497, Class Accuracy: 11.399999618530273, Domain Accuracy: 50.0\n",
      "Loss: 2.9957289695739746, Class Accuracy: 10.727272987365723, Domain Accuracy: 50.0\n",
      "Loss: 2.9957282543182373, Class Accuracy: 10.833333015441895, Domain Accuracy: 50.0\n",
      "Loss: 2.9957287311553955, Class Accuracy: 10.615385055541992, Domain Accuracy: 50.0\n",
      "Loss: 2.995716094970703, Class Accuracy: 11.428571701049805, Domain Accuracy: 50.0\n",
      "Loss: 2.9957115650177, Class Accuracy: 11.333333969116211, Domain Accuracy: 50.0\n",
      "Loss: 2.995727777481079, Class Accuracy: 11.0, Domain Accuracy: 50.0\n",
      "Loss: 2.995731830596924, Class Accuracy: 10.70588207244873, Domain Accuracy: 50.0\n",
      "Loss: 2.995730400085449, Class Accuracy: 10.55555534362793, Domain Accuracy: 50.0\n",
      "Loss: 2.9957275390625, Class Accuracy: 10.631579399108887, Domain Accuracy: 50.0\n",
      "Loss: 2.995718479156494, Class Accuracy: 10.59999942779541, Domain Accuracy: 50.0\n",
      "Loss: 2.995722532272339, Class Accuracy: 10.380951881408691, Domain Accuracy: 50.0\n",
      "Loss: 2.9957163333892822, Class Accuracy: 10.636363983154297, Domain Accuracy: 50.0\n",
      "Loss: 2.9957199096679688, Class Accuracy: 10.608695030212402, Domain Accuracy: 50.0\n",
      "Loss: 2.9957196712493896, Class Accuracy: 10.416666030883789, Domain Accuracy: 50.0\n",
      "Loss: 2.9957220554351807, Class Accuracy: 10.40000057220459, Domain Accuracy: 50.0\n",
      "Loss: 2.995730400085449, Class Accuracy: 10.30769157409668, Domain Accuracy: 50.0\n",
      "Loss: 2.995737314224243, Class Accuracy: 10.296296119689941, Domain Accuracy: 50.0\n",
      "Loss: 2.9957387447357178, Class Accuracy: 10.285714149475098, Domain Accuracy: 50.0\n",
      "Loss: 2.995730400085449, Class Accuracy: 10.758620262145996, Domain Accuracy: 50.0\n",
      "Loss: 2.995727777481079, Class Accuracy: 10.733333587646484, Domain Accuracy: 50.0\n",
      "Loss: 2.9957191944122314, Class Accuracy: 10.580645561218262, Domain Accuracy: 50.0\n",
      "Loss: 2.995720624923706, Class Accuracy: 10.375, Domain Accuracy: 50.0\n",
      "Loss: 2.9957237243652344, Class Accuracy: 10.42424201965332, Domain Accuracy: 50.0\n",
      "Loss: 2.995727300643921, Class Accuracy: 10.470588684082031, Domain Accuracy: 50.0\n",
      "Loss: 2.995725154876709, Class Accuracy: 10.742856979370117, Domain Accuracy: 50.0\n",
      "Loss: 2.9957242012023926, Class Accuracy: 10.888888359069824, Domain Accuracy: 50.0\n",
      "Loss: 2.9957242012023926, Class Accuracy: 10.972972869873047, Domain Accuracy: 50.0\n",
      "Loss: 2.995727777481079, Class Accuracy: 10.8421049118042, Domain Accuracy: 50.0\n",
      "Loss: 2.9957265853881836, Class Accuracy: 10.769230842590332, Domain Accuracy: 50.0\n",
      "Loss: 2.995730400085449, Class Accuracy: 10.800000190734863, Domain Accuracy: 50.0\n",
      "Loss: 2.995732069015503, Class Accuracy: 10.87804889678955, Domain Accuracy: 50.0\n",
      "Loss: 2.9957315921783447, Class Accuracy: 10.952381134033203, Domain Accuracy: 50.0\n",
      "Loss: 2.9957261085510254, Class Accuracy: 10.976744651794434, Domain Accuracy: 50.0\n",
      "Loss: 2.995729923248291, Class Accuracy: 11.0, Domain Accuracy: 50.0\n",
      "Loss: 2.995727777481079, Class Accuracy: 11.022222518920898, Domain Accuracy: 50.0\n",
      "Loss: 2.9957218170166016, Class Accuracy: 11.1304349899292, Domain Accuracy: 50.0\n",
      "Loss: 2.99570894241333, Class Accuracy: 11.23404312133789, Domain Accuracy: 50.0\n",
      "Loss: 2.995708465576172, Class Accuracy: 11.291666030883789, Domain Accuracy: 50.0\n",
      "Loss: 2.9957027435302734, Class Accuracy: 11.306122779846191, Domain Accuracy: 50.0\n",
      "Loss: 2.9957034587860107, Class Accuracy: 11.319999694824219, Domain Accuracy: 50.0\n",
      "Loss: 2.995699167251587, Class Accuracy: 11.372549057006836, Domain Accuracy: 50.0\n",
      "Loss: 2.995687961578369, Class Accuracy: 11.423076629638672, Domain Accuracy: 50.0\n",
      "Loss: 2.995690107345581, Class Accuracy: 11.433961868286133, Domain Accuracy: 50.0\n",
      "Loss: 2.9956884384155273, Class Accuracy: 11.333333969116211, Domain Accuracy: 50.0\n",
      "Loss: 2.9956743717193604, Class Accuracy: 11.454545974731445, Domain Accuracy: 50.0\n",
      "Loss: 2.9956719875335693, Class Accuracy: 11.357142448425293, Domain Accuracy: 50.0\n",
      "Loss: 2.9956703186035156, Class Accuracy: 11.368420600891113, Domain Accuracy: 50.0\n",
      "Loss: 2.995664596557617, Class Accuracy: 11.482758522033691, Domain Accuracy: 50.0\n",
      "Loss: 2.995656728744507, Class Accuracy: 11.559321403503418, Domain Accuracy: 50.0\n",
      "Loss: 2.9956538677215576, Class Accuracy: 11.566666603088379, Domain Accuracy: 50.0\n",
      "Loss: 2.995645046234131, Class Accuracy: 11.573770523071289, Domain Accuracy: 50.0\n",
      "Loss: 2.9956541061401367, Class Accuracy: 11.451613426208496, Domain Accuracy: 50.0\n",
      "Loss: 2.9956483840942383, Class Accuracy: 11.428571701049805, Domain Accuracy: 50.0\n",
      "Loss: 2.995638847351074, Class Accuracy: 11.46875, Domain Accuracy: 50.0\n",
      "Loss: 2.9956305027008057, Class Accuracy: 11.538461685180664, Domain Accuracy: 50.0\n",
      "Loss: 2.9956326484680176, Class Accuracy: 11.606060981750488, Domain Accuracy: 50.0\n",
      "Loss: 2.9956247806549072, Class Accuracy: 11.641790390014648, Domain Accuracy: 50.0\n",
      "Loss: 2.9956421852111816, Class Accuracy: 11.558823585510254, Domain Accuracy: 50.0\n",
      "Loss: 2.9956367015838623, Class Accuracy: 11.594202995300293, Domain Accuracy: 50.0\n",
      "Loss: 2.995640277862549, Class Accuracy: 11.54285717010498, Domain Accuracy: 50.0\n",
      "Loss: 2.9956462383270264, Class Accuracy: 11.521126747131348, Domain Accuracy: 50.0\n",
      "Loss: 2.99565052986145, Class Accuracy: 11.527777671813965, Domain Accuracy: 50.0\n",
      "Loss: 2.995649814605713, Class Accuracy: 11.50684928894043, Domain Accuracy: 50.0\n",
      "Loss: 2.9956552982330322, Class Accuracy: 11.405405044555664, Domain Accuracy: 50.0\n",
      "Loss: 2.9956538677215576, Class Accuracy: 11.49333381652832, Domain Accuracy: 50.0\n",
      "Loss: 2.995654821395874, Class Accuracy: 11.421051979064941, Domain Accuracy: 50.0\n",
      "Loss: 2.9956552982330322, Class Accuracy: 11.324675559997559, Domain Accuracy: 50.0\n",
      "Loss: 2.9956634044647217, Class Accuracy: 11.205127716064453, Domain Accuracy: 50.0\n",
      "Loss: 2.995661973953247, Class Accuracy: 11.240506172180176, Domain Accuracy: 50.0\n",
      "Loss: 2.9956679344177246, Class Accuracy: 11.175000190734863, Domain Accuracy: 50.0\n",
      "Loss: 2.995666980743408, Class Accuracy: 11.209877014160156, Domain Accuracy: 50.0\n",
      "Loss: 2.995669364929199, Class Accuracy: 11.195121765136719, Domain Accuracy: 50.0\n",
      "Loss: 2.9956700801849365, Class Accuracy: 11.204819679260254, Domain Accuracy: 50.0\n",
      "Loss: 2.995663642883301, Class Accuracy: 11.190476417541504, Domain Accuracy: 50.0\n",
      "Loss: 2.995662212371826, Class Accuracy: 11.200000762939453, Domain Accuracy: 50.0\n",
      "Loss: 2.995659828186035, Class Accuracy: 11.162790298461914, Domain Accuracy: 50.0\n",
      "Loss: 2.9956655502319336, Class Accuracy: 11.103447914123535, Domain Accuracy: 50.0\n",
      "Loss: 2.995669364929199, Class Accuracy: 11.068181991577148, Domain Accuracy: 50.0\n",
      "Loss: 2.995668888092041, Class Accuracy: 10.988763809204102, Domain Accuracy: 50.0\n",
      "Loss: 2.99566650390625, Class Accuracy: 10.977777481079102, Domain Accuracy: 50.0\n",
      "Loss: 2.9956624507904053, Class Accuracy: 11.05494499206543, Domain Accuracy: 50.0\n",
      "Loss: 2.9956705570220947, Class Accuracy: 11.0, Domain Accuracy: 50.0\n",
      "Loss: 2.995676279067993, Class Accuracy: 10.924731254577637, Domain Accuracy: 50.0\n",
      "Loss: 2.995678186416626, Class Accuracy: 10.91489315032959, Domain Accuracy: 50.0\n",
      "Loss: 2.9956774711608887, Class Accuracy: 10.926316261291504, Domain Accuracy: 50.0\n",
      "Loss: 2.9956729412078857, Class Accuracy: 10.979166984558105, Domain Accuracy: 50.0\n",
      "Loss: 2.9956722259521484, Class Accuracy: 10.948453903198242, Domain Accuracy: 50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.9956729412078857, Class Accuracy: 10.918367385864258, Domain Accuracy: 50.0\n",
      "Loss: 2.995661497116089, Class Accuracy: 11.010100364685059, Domain Accuracy: 50.0\n",
      "Loss: 2.9956610202789307, Class Accuracy: 10.980000495910645, Domain Accuracy: 50.0\n",
      "Loss: 2.9956724643707275, Class Accuracy: 10.930692672729492, Domain Accuracy: 50.0\n",
      "Loss: 2.9956724643707275, Class Accuracy: 10.960784912109375, Domain Accuracy: 50.0\n",
      "Loss: 2.9956648349761963, Class Accuracy: 10.990290641784668, Domain Accuracy: 50.0\n",
      "Loss: 2.995664358139038, Class Accuracy: 11.0, Domain Accuracy: 50.0\n",
      "Loss: 2.9956586360931396, Class Accuracy: 11.047618865966797, Domain Accuracy: 50.0\n",
      "Loss: 2.9956579208374023, Class Accuracy: 11.037735939025879, Domain Accuracy: 50.0\n",
      "Loss: 2.995654582977295, Class Accuracy: 11.04672908782959, Domain Accuracy: 50.0\n",
      "Loss: 2.9956624507904053, Class Accuracy: 11.0, Domain Accuracy: 50.0\n",
      "Loss: 2.9956650733947754, Class Accuracy: 11.064220428466797, Domain Accuracy: 50.0\n",
      "Loss: 2.995664358139038, Class Accuracy: 11.018181800842285, Domain Accuracy: 50.0\n",
      "Loss: 2.995671272277832, Class Accuracy: 10.99099063873291, Domain Accuracy: 50.0\n",
      "Loss: 2.9956610202789307, Class Accuracy: 11.035714149475098, Domain Accuracy: 50.0\n",
      "Loss: 2.9956483840942383, Class Accuracy: 11.115044593811035, Domain Accuracy: 50.0\n",
      "Loss: 2.995638370513916, Class Accuracy: 11.1578950881958, Domain Accuracy: 50.0\n",
      "Loss: 2.995640277862549, Class Accuracy: 11.147826194763184, Domain Accuracy: 50.0\n",
      "Loss: 2.9956419467926025, Class Accuracy: 11.103447914123535, Domain Accuracy: 50.0\n",
      "Loss: 2.9956464767456055, Class Accuracy: 11.076922416687012, Domain Accuracy: 50.0\n",
      "Loss: 2.9956400394439697, Class Accuracy: 11.084745407104492, Domain Accuracy: 50.0\n",
      "Loss: 2.995635509490967, Class Accuracy: 11.075630187988281, Domain Accuracy: 50.0\n",
      "Loss: 2.995629072189331, Class Accuracy: 11.066666603088379, Domain Accuracy: 50.0\n",
      "Loss: 2.9956376552581787, Class Accuracy: 11.008264541625977, Domain Accuracy: 50.0\n",
      "Loss: 2.995638370513916, Class Accuracy: 11.049180030822754, Domain Accuracy: 50.0\n",
      "Loss: 2.995640993118286, Class Accuracy: 11.07317066192627, Domain Accuracy: 50.0\n",
      "Loss: 2.9956464767456055, Class Accuracy: 11.080644607543945, Domain Accuracy: 50.0\n",
      "Loss: 2.9956374168395996, Class Accuracy: 11.13599967956543, Domain Accuracy: 50.0\n",
      "Loss: 2.9956390857696533, Class Accuracy: 11.158730506896973, Domain Accuracy: 50.0\n",
      "Loss: 2.9956412315368652, Class Accuracy: 11.133858680725098, Domain Accuracy: 50.0\n",
      "Loss: 2.99564528465271, Class Accuracy: 11.125, Domain Accuracy: 50.0\n",
      "Loss: 2.9956512451171875, Class Accuracy: 11.069766998291016, Domain Accuracy: 50.0\n",
      "Loss: 2.9956467151641846, Class Accuracy: 11.076922416687012, Domain Accuracy: 50.0\n",
      "Loss: 2.9956440925598145, Class Accuracy: 11.083969116210938, Domain Accuracy: 50.0\n",
      "Loss: 2.9956307411193848, Class Accuracy: 11.166666030883789, Domain Accuracy: 50.0\n",
      "Loss: 2.9956214427948, Class Accuracy: 11.218045234680176, Domain Accuracy: 50.0\n",
      "Loss: 2.9956281185150146, Class Accuracy: 11.164179801940918, Domain Accuracy: 50.0\n",
      "Loss: 2.99562931060791, Class Accuracy: 11.155555725097656, Domain Accuracy: 50.0\n",
      "Loss: 2.9956226348876953, Class Accuracy: 11.25, Domain Accuracy: 50.0\n",
      "Loss: 2.995617389678955, Class Accuracy: 11.240875244140625, Domain Accuracy: 50.0\n",
      "Loss: 2.995619297027588, Class Accuracy: 11.246376037597656, Domain Accuracy: 50.0\n",
      "Loss: 2.995616912841797, Class Accuracy: 11.251798629760742, Domain Accuracy: 50.0\n",
      "Loss: 2.9956185817718506, Class Accuracy: 11.271428108215332, Domain Accuracy: 50.0\n",
      "Loss: 2.9956154823303223, Class Accuracy: 11.319149017333984, Domain Accuracy: 50.0\n",
      "Loss: 2.995604991912842, Class Accuracy: 11.40845012664795, Domain Accuracy: 50.0\n",
      "Loss: 2.995598077774048, Class Accuracy: 11.48251724243164, Domain Accuracy: 50.0\n",
      "Loss: 2.9955949783325195, Class Accuracy: 11.5, Domain Accuracy: 50.0\n",
      "Loss: 2.9955921173095703, Class Accuracy: 11.503448486328125, Domain Accuracy: 50.0\n",
      "Loss: 2.995591163635254, Class Accuracy: 11.534246444702148, Domain Accuracy: 50.0\n",
      "Loss: 2.9955928325653076, Class Accuracy: 11.51020336151123, Domain Accuracy: 50.0\n",
      "Loss: 2.9955878257751465, Class Accuracy: 11.486486434936523, Domain Accuracy: 50.0\n",
      "Loss: 2.995591640472412, Class Accuracy: 11.46308708190918, Domain Accuracy: 50.0\n",
      "Loss: 2.995600461959839, Class Accuracy: 11.453333854675293, Domain Accuracy: 50.0\n",
      "Loss: 2.995602607727051, Class Accuracy: 11.430463790893555, Domain Accuracy: 50.0\n",
      "Loss: 2.995605707168579, Class Accuracy: 11.394737243652344, Domain Accuracy: 50.0\n",
      "Loss: 2.9956064224243164, Class Accuracy: 11.385621070861816, Domain Accuracy: 50.0\n",
      "Loss: 2.9956095218658447, Class Accuracy: 11.36363697052002, Domain Accuracy: 50.0\n",
      "Loss: 2.9956133365631104, Class Accuracy: 11.354839324951172, Domain Accuracy: 50.0\n",
      "Loss: 2.9956135749816895, Class Accuracy: 11.371794700622559, Domain Accuracy: 50.0\n",
      "Loss: 2.9956140518188477, Class Accuracy: 11.3757963180542, Domain Accuracy: 50.0\n",
      "Loss: 2.9956088066101074, Class Accuracy: 11.40506362915039, Domain Accuracy: 50.0\n",
      "Loss: 2.995600938796997, Class Accuracy: 11.446540832519531, Domain Accuracy: 50.0\n",
      "Loss: 2.9956014156341553, Class Accuracy: 11.424999237060547, Domain Accuracy: 50.0\n",
      "Loss: 2.995591878890991, Class Accuracy: 11.465838432312012, Domain Accuracy: 50.0\n",
      "Loss: 2.9955921173095703, Class Accuracy: 11.469135284423828, Domain Accuracy: 50.0\n",
      "Loss: 2.9955837726593018, Class Accuracy: 11.496932983398438, Domain Accuracy: 50.0\n",
      "Loss: 2.99558424949646, Class Accuracy: 11.5, Domain Accuracy: 50.0\n",
      "Loss: 2.99558424949646, Class Accuracy: 11.4909086227417, Domain Accuracy: 50.0\n",
      "Loss: 2.9955713748931885, Class Accuracy: 11.542168617248535, Domain Accuracy: 50.0\n",
      "Loss: 2.995560884475708, Class Accuracy: 11.568861961364746, Domain Accuracy: 50.0\n",
      "Loss: 2.9955592155456543, Class Accuracy: 11.595237731933594, Domain Accuracy: 50.0\n",
      "Loss: 2.9955520629882812, Class Accuracy: 11.609467506408691, Domain Accuracy: 50.0\n",
      "Loss: 2.995546340942383, Class Accuracy: 11.611764907836914, Domain Accuracy: 50.0\n",
      "Loss: 2.9955453872680664, Class Accuracy: 11.614034652709961, Domain Accuracy: 50.0\n",
      "Loss: 2.995542287826538, Class Accuracy: 11.60465145111084, Domain Accuracy: 50.0\n",
      "Loss: 2.995539903640747, Class Accuracy: 11.618496894836426, Domain Accuracy: 50.0\n",
      "Loss: 2.9955382347106934, Class Accuracy: 11.62069034576416, Domain Accuracy: 50.0\n",
      "Loss: 2.995542287826538, Class Accuracy: 11.565713882446289, Domain Accuracy: 50.0\n",
      "Loss: 2.995549201965332, Class Accuracy: 11.522727012634277, Domain Accuracy: 50.0\n",
      "Loss: 2.9955523014068604, Class Accuracy: 11.514123916625977, Domain Accuracy: 50.0\n",
      "Loss: 2.995558261871338, Class Accuracy: 11.49438190460205, Domain Accuracy: 50.0\n",
      "Loss: 2.9955577850341797, Class Accuracy: 11.508379936218262, Domain Accuracy: 50.0\n",
      "Loss: 2.9955570697784424, Class Accuracy: 11.5, Domain Accuracy: 50.0\n",
      "Loss: 2.995558261871338, Class Accuracy: 11.480663299560547, Domain Accuracy: 50.0\n",
      "Loss: 2.9955599308013916, Class Accuracy: 11.472527503967285, Domain Accuracy: 50.0\n",
      "Loss: 2.995565414428711, Class Accuracy: 11.45355224609375, Domain Accuracy: 50.0\n",
      "Loss: 2.9955642223358154, Class Accuracy: 11.456521987915039, Domain Accuracy: 50.0\n",
      "Loss: 2.9955573081970215, Class Accuracy: 11.481081008911133, Domain Accuracy: 50.0\n",
      "Loss: 2.9955523014068604, Class Accuracy: 11.516129493713379, Domain Accuracy: 50.0\n",
      "Loss: 2.995549440383911, Class Accuracy: 11.518716812133789, Domain Accuracy: 50.0\n",
      "Loss: 2.9955480098724365, Class Accuracy: 11.531914710998535, Domain Accuracy: 50.0\n",
      "Loss: 2.9955456256866455, Class Accuracy: 11.544973373413086, Domain Accuracy: 50.0\n",
      "Loss: 2.9955413341522217, Class Accuracy: 11.59999942779541, Domain Accuracy: 50.0\n",
      "Loss: 2.9955434799194336, Class Accuracy: 11.591623306274414, Domain Accuracy: 50.0\n",
      "Loss: 2.995548963546753, Class Accuracy: 11.572916984558105, Domain Accuracy: 50.0\n",
      "Loss: 2.9955554008483887, Class Accuracy: 11.544041633605957, Domain Accuracy: 50.0\n",
      "Loss: 2.9955549240112305, Class Accuracy: 11.53608226776123, Domain Accuracy: 50.0\n",
      "Loss: 2.9955384731292725, Class Accuracy: 11.59999942779541, Domain Accuracy: 50.0\n",
      "Loss: 2.995541572570801, Class Accuracy: 11.571429252624512, Domain Accuracy: 50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.9955391883850098, Class Accuracy: 11.563451766967773, Domain Accuracy: 50.0\n",
      "Loss: 2.995540142059326, Class Accuracy: 11.535353660583496, Domain Accuracy: 50.0\n",
      "Loss: 2.995542526245117, Class Accuracy: 11.507537841796875, Domain Accuracy: 50.0\n",
      "Loss: 2.9955430030822754, Class Accuracy: 11.489999771118164, Domain Accuracy: 50.0\n",
      "Loss: 2.9955356121063232, Class Accuracy: 11.502487182617188, Domain Accuracy: 50.0\n",
      "Loss: 2.99552845954895, Class Accuracy: 11.495049476623535, Domain Accuracy: 50.0\n",
      "Loss: 2.9955272674560547, Class Accuracy: 11.517241477966309, Domain Accuracy: 50.0\n",
      "Loss: 2.9955241680145264, Class Accuracy: 11.529412269592285, Domain Accuracy: 50.0\n",
      "Loss: 2.9955224990844727, Class Accuracy: 11.531707763671875, Domain Accuracy: 50.0\n",
      "Loss: 2.995526075363159, Class Accuracy: 11.504854202270508, Domain Accuracy: 50.0\n",
      "Loss: 2.995529890060425, Class Accuracy: 11.507246971130371, Domain Accuracy: 50.0\n",
      "Loss: 2.9955291748046875, Class Accuracy: 11.480769157409668, Domain Accuracy: 50.0\n",
      "Loss: 2.995523691177368, Class Accuracy: 11.492822647094727, Domain Accuracy: 50.0\n",
      "Loss: 2.995514392852783, Class Accuracy: 11.533333778381348, Domain Accuracy: 50.0\n",
      "Loss: 2.995523691177368, Class Accuracy: 11.507108688354492, Domain Accuracy: 50.0\n",
      "Loss: 2.9955246448516846, Class Accuracy: 11.49056625366211, Domain Accuracy: 50.0\n",
      "Loss: 2.995518684387207, Class Accuracy: 11.511736869812012, Domain Accuracy: 50.0\n",
      "Loss: 2.9955177307128906, Class Accuracy: 11.50467300415039, Domain Accuracy: 50.0\n",
      "Loss: 2.9955081939697266, Class Accuracy: 11.525581359863281, Domain Accuracy: 50.0\n",
      "Loss: 2.995513916015625, Class Accuracy: 11.490740776062012, Domain Accuracy: 50.0\n",
      "Loss: 2.995513439178467, Class Accuracy: 11.465437889099121, Domain Accuracy: 50.0\n",
      "Loss: 2.995515823364258, Class Accuracy: 11.47706413269043, Domain Accuracy: 50.0\n",
      "Loss: 2.995511531829834, Class Accuracy: 11.488584518432617, Domain Accuracy: 50.0\n",
      "Loss: 2.9955027103424072, Class Accuracy: 11.545454978942871, Domain Accuracy: 50.0\n",
      "Loss: 2.9955005645751953, Class Accuracy: 11.529412269592285, Domain Accuracy: 50.0\n",
      "Loss: 2.9954993724823, Class Accuracy: 11.531532287597656, Domain Accuracy: 50.0\n",
      "Loss: 2.9955074787139893, Class Accuracy: 11.533632278442383, Domain Accuracy: 50.0\n",
      "Loss: 2.995511770248413, Class Accuracy: 11.51785659790039, Domain Accuracy: 50.0\n",
      "Loss: 2.9955132007598877, Class Accuracy: 11.51111125946045, Domain Accuracy: 50.0\n",
      "Loss: 2.9955174922943115, Class Accuracy: 11.486725807189941, Domain Accuracy: 50.0\n",
      "Loss: 2.995516777038574, Class Accuracy: 11.471364974975586, Domain Accuracy: 50.0\n",
      "Loss: 2.9955148696899414, Class Accuracy: 11.48245620727539, Domain Accuracy: 50.0\n",
      "Loss: 2.995506763458252, Class Accuracy: 11.502182960510254, Domain Accuracy: 50.0\n",
      "Loss: 2.9955029487609863, Class Accuracy: 11.530435562133789, Domain Accuracy: 50.0\n",
      "Loss: 2.9954910278320312, Class Accuracy: 11.57575798034668, Domain Accuracy: 50.0\n",
      "Loss: 2.9954886436462402, Class Accuracy: 11.586207389831543, Domain Accuracy: 50.0\n",
      "Loss: 2.995497703552246, Class Accuracy: 11.570816040039062, Domain Accuracy: 50.0\n",
      "Loss: 2.9954946041107178, Class Accuracy: 11.589743614196777, Domain Accuracy: 50.0\n",
      "Loss: 2.9954957962036133, Class Accuracy: 11.574467658996582, Domain Accuracy: 50.0\n",
      "Loss: 2.995493173599243, Class Accuracy: 11.593220710754395, Domain Accuracy: 50.0\n",
      "Loss: 2.9954941272735596, Class Accuracy: 11.569620132446289, Domain Accuracy: 50.0\n",
      "Loss: 2.9955008029937744, Class Accuracy: 11.546218872070312, Domain Accuracy: 50.0\n",
      "Loss: 2.9955029487609863, Class Accuracy: 11.539749145507812, Domain Accuracy: 50.0\n",
      "Loss: 2.9954960346221924, Class Accuracy: 11.574999809265137, Domain Accuracy: 50.0\n",
      "Loss: 2.9954986572265625, Class Accuracy: 11.560165405273438, Domain Accuracy: 50.0\n",
      "Loss: 2.99550199508667, Class Accuracy: 11.561984062194824, Domain Accuracy: 50.0\n",
      "Loss: 2.9955031871795654, Class Accuracy: 11.547325134277344, Domain Accuracy: 50.0\n",
      "Loss: 2.9955050945281982, Class Accuracy: 11.532787322998047, Domain Accuracy: 50.0\n",
      "Loss: 2.995504140853882, Class Accuracy: 11.518367767333984, Domain Accuracy: 50.0\n",
      "Loss: 2.9955101013183594, Class Accuracy: 11.50406551361084, Domain Accuracy: 50.0\n",
      "Loss: 2.9955127239227295, Class Accuracy: 11.481781005859375, Domain Accuracy: 50.0\n",
      "Loss: 2.9955027103424072, Class Accuracy: 11.516129493713379, Domain Accuracy: 50.0\n",
      "Loss: 2.9954946041107178, Class Accuracy: 11.558233261108398, Domain Accuracy: 50.0\n",
      "Loss: 2.995488166809082, Class Accuracy: 11.567999839782715, Domain Accuracy: 50.0\n",
      "Loss: 2.995485782623291, Class Accuracy: 11.561753273010254, Domain Accuracy: 50.0\n",
      "Loss: 2.9954800605773926, Class Accuracy: 11.563491821289062, Domain Accuracy: 50.0\n",
      "Loss: 2.9954779148101807, Class Accuracy: 11.55731201171875, Domain Accuracy: 50.0\n",
      "Loss: 2.995473861694336, Class Accuracy: 11.574803352355957, Domain Accuracy: 50.0\n",
      "Loss: 2.995466947555542, Class Accuracy: 11.592157363891602, Domain Accuracy: 50.0\n",
      "Loss: 2.995461940765381, Class Accuracy: 11.6015625, Domain Accuracy: 50.0\n",
      "Loss: 2.9954657554626465, Class Accuracy: 11.587549209594727, Domain Accuracy: 50.0\n",
      "Loss: 2.9954681396484375, Class Accuracy: 11.589147567749023, Domain Accuracy: 50.0\n",
      "Loss: 2.995469331741333, Class Accuracy: 11.567566871643066, Domain Accuracy: 50.0\n",
      "Loss: 2.995471239089966, Class Accuracy: 11.561538696289062, Domain Accuracy: 50.0\n",
      "Loss: 2.995483160018921, Class Accuracy: 11.524904251098633, Domain Accuracy: 50.0\n",
      "Loss: 2.9954826831817627, Class Accuracy: 11.519083976745605, Domain Accuracy: 50.0\n",
      "Loss: 2.995483636856079, Class Accuracy: 11.528517723083496, Domain Accuracy: 50.0\n",
      "Loss: 2.9954793453216553, Class Accuracy: 11.560606002807617, Domain Accuracy: 50.0\n",
      "Loss: 2.9954702854156494, Class Accuracy: 11.592453002929688, Domain Accuracy: 50.0\n",
      "Loss: 2.995469808578491, Class Accuracy: 11.593984603881836, Domain Accuracy: 50.0\n",
      "Loss: 2.995469093322754, Class Accuracy: 11.588014602661133, Domain Accuracy: 50.0\n",
      "Loss: 2.9954659938812256, Class Accuracy: 11.589552879333496, Domain Accuracy: 50.0\n",
      "Loss: 2.9954605102539062, Class Accuracy: 11.591078758239746, Domain Accuracy: 50.0\n",
      "Loss: 2.995460271835327, Class Accuracy: 11.59999942779541, Domain Accuracy: 50.0\n",
      "Loss: 2.995450735092163, Class Accuracy: 11.616235733032227, Domain Accuracy: 50.0\n",
      "Loss: 2.9954569339752197, Class Accuracy: 11.595587730407715, Domain Accuracy: 50.0\n",
      "Loss: 2.995455503463745, Class Accuracy: 11.575091361999512, Domain Accuracy: 50.0\n",
      "Loss: 2.9954686164855957, Class Accuracy: 11.540145874023438, Domain Accuracy: 50.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ea66cd76916c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Loss: {}, Class Accuracy: {}, Domain Accuracy: {}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_acc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_acc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    895\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1178\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6861\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   6862\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6863\u001b[1;33m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[0;32m   6864\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6865\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(data_feeder.num_batch)):        \n",
    "        [data, domain_data, class_label, domain_label] = data_feeder.feed()\n",
    "#         (class_pred, domain_pred) = model.predict(data, domain_data)\n",
    "#         loss = model.calculate_loss((class_label, domain_label), (class_pred, domain_pred))        \n",
    "#         (m_loss, c_acc, d_acc) = model.calculate_log(loss, class_label, domain_label, class_pred, domain_pred)\n",
    "        \n",
    "        (m_loss, c_acc, d_acc) = model.fit(data, domain_data, (class_label, domain_label))\n",
    "\n",
    "    #     for test_images, test_labels in test_ds:\n",
    "    #         test_step(test_images, test_labels)\n",
    "\n",
    "        template = 'Loss: {}, Class Accuracy: {}, Domain Accuracy: {}'\n",
    "        print(template.format(m_loss, c_acc*100, d_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
